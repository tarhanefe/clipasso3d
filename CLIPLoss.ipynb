{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c2c891",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m target \u001b[38;5;241m=\u001b[39m get_dummy_image()\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Instantiate and run the combined Loss\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m losses_dict \u001b[38;5;241m=\u001b[39m loss_fn(sketch, target, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute total loss from weighted components\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/deneme/clipasso3d/./source/cliploss.py:26\u001b[0m, in \u001b[0;36mLoss.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_text_guide \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mclip_text_guide\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses_to_apply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_losses_to_apply()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_mapper \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     25\u001b[0m     {\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mCLIPLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_conv_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: CLIPConvLoss(args)\n\u001b[1;32m     28\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/deneme/clipasso3d/./source/cliploss.py:84\u001b[0m, in \u001b[0;36mCLIPLoss.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28msuper\u001b[39m(CLIPLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, clip_preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mViT-B/32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     88\u001b[0m     [clip_preprocess\u001b[38;5;241m.\u001b[39mtransforms[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])  \u001b[38;5;66;03m# clip normalisation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/deneme/clipasso3d/CLIP_/clip/clip.py:120\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a CLIP model\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[0;32m--> 120\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/.cache/clip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(name):\n\u001b[1;32m    122\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/Desktop/deneme/clipasso3d/CLIP_/clip/clip.py:54\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(url, root)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists and is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(download_target):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhashlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msha256\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m expected_sha256:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m download_target\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "\n",
    "# Add the source folder to sys.path\n",
    "sys.path.append('./source')\n",
    "\n",
    "# Import the top-level Loss class\n",
    "from cliploss import Loss\n",
    "\n",
    "# Define dummy args with both semantic and geometric loss disabled/enabled as needed\n",
    "args = SimpleNamespace(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    percep_loss=\"none\",                  # no perceptual loss like LPIPS\n",
    "    train_with_clip=False,               # enable CLIP loss\n",
    "    clip_weight=0,                    # weight for semantic CLIP loss\n",
    "    start_clip=0,                       # start CLIP loss immediately\n",
    "    clip_conv_loss=1,              # disable geometric loss for this test\n",
    "    clip_fc_loss_weight=0.1,           # weight for fc in conv loss (irrelevant since disabled)\n",
    "    clip_text_guide=0.0,               # unused here\n",
    "    num_aug_clip=4,                    # number of augmentations\n",
    "    augemntations=[\"affine\"],          # apply affine augmentation\n",
    "    include_target_in_aug=False,       # only augment sketch\n",
    "    augment_both=False,\n",
    "    clip_model_name=\"ViT-B/32\",\n",
    "    clip_conv_loss_type=\"L2\",          # not used in this config\n",
    "    clip_conv_layer_weights=[0, 0, 1.0, 1.0, 0],  # not used unless clip_conv_loss=True\n",
    ")\n",
    "\n",
    "# Create dummy sketch and target tensors\n",
    "def get_dummy_image(size=(224, 224)):\n",
    "    return torch.rand(1, 3, *size)\n",
    "\n",
    "sketch = get_dummy_image().to(args.device)\n",
    "target = get_dummy_image().to(args.device)\n",
    "\n",
    "# Instantiate and run the combined Loss\n",
    "loss_fn = Loss(args).to(args.device)\n",
    "losses_dict = loss_fn(sketch, target, epoch=100, mode=\"train\")\n",
    "\n",
    "# Compute total loss from weighted components\n",
    "final_loss = sum(losses_dict.values())\n",
    "\n",
    "# Print results\n",
    "print(\"Loss breakdown:\")\n",
    "for name, val in losses_dict.items():\n",
    "    print(f\"  {name}: {val.item():.4f}\")\n",
    "print(f\"Total weighted loss: {final_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cca59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vimm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
